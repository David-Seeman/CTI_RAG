{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"sk-af8V6rBrwtDdr6h2gD4OGw\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://api.ai.it.cornell.edu\" # https://<XXX>.openai.azure.com/\"\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt-4o\",\n",
    "    temperature=0.2,\n",
    "    api_version=\"2023-06-01-preview\",\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Bob! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 11, 'total_tokens': 21, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_d54531d9eb', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {}}, id='run-8903eca2-4d3e-41e6-ae09-67cb4a54134b-0', usage_metadata={'input_tokens': 11, 'output_tokens': 10, 'total_tokens': 21, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm.invoke([HumanMessage(content=\"Hi! I'm Bob\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Load Source Text</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"cornell.txt\") # \"/workspace/data/knowledge_base/fruits_and_veggies.txt\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'cornell.txt'}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cornell University\n",
      "20th Century History\n",
      "In 1967, Cornell experienced a fire in the Residential Club dormitory that killed eight students and one professor. In the late 1960s, Cornell was among the Ivy League universities that experienced heightened student activism related to cultural issues, civil rights, and opposition to U.S. involvement in the Vietnam War. In 1969, armed anti-Vietnam War protesters occupied Willard Straight Hall, an incident that led to a restructuring of the university's governance and forced the resignation of then Cornell president James Alfred Perkins.\n",
      "Since the 20th century, rankings of universities and colleges, Cornell University and its academic programs have routinely ranked among the best in the world. In 1995, the National Research Council ranked Cornell's Ph.D. programs as sixth-best in the nation. It also ranked the academic quality of 18 individual Cornell Ph.D. programs among the top ten in the nation, which included astrophysics (ninth-best), chemistry (sixth-best), civil engineering (sixth-best), comparative literature (sixth-best), computer science (fifth-best), ecology (fourth-best), electrical engineering (seventh-best), English (seventh-best), French (eighth-best), geosciences (tenth-best), German (third-best), linguistics (ninth-best), materials science (third-best), mechanical engineering (seventh-best), philosophy (ninth-best), physics (sixth-best), Spanish (eighth-best), and statistics/biostatistics (fourth-best). The council ranked Cornell's College of Arts and Humanities faculty as fifth-best in the nation, its Mathematics and Physical Sciences faculty as sixth-best, and its College of Engineering as fifth-best.\n",
      "Natural Surroundings\n",
      "Cornell University's main campus in Ithaca is located in the Finger Lakes region in upstate New York and features views of the city, Cayuga Lake, and surrounding valleys. The campus is bordered by two gorges, Fall Creek Gorge and Cascadilla Gorge. The gorges are popular swimming spots during warmer months, but their use is discouraged by the university and city code due to potential safety hazards. Adjacent to the main campus, Cornell owns the 2,800-acre (1,100 ha) Cornell Botanic Gardens, which features various plants, trees, and ponds.\n",
      "Sustainability\n",
      "Cornell University has implemented several green initiatives, designed to promote sustainability and reduce environmental impact, including a gas-fired combined heat and power facility,an on-campus hydroelectric plant, and a lake source cooling system. In 2007, Cornell established a Center for a Sustainable Future. The same year, following a multiyear, cross-campus discussion about energy and sustainability, Cornell's Atkinson Center for Sustainability was established, funded by an $80 million gift from alumnus David R. Atkinson ('60) and his wife Patricia, the largest gift ever received by Cornell from an individual at the time. A subsequent $30 million commitment in 2021 will name a new multidisciplinary building on campus.\n",
      "As of 2020, the university, which has committed to achieving net carbon neutrality by 2035 is powered by six solar farms, which provide 28 megawatts of power. Cornell is developing an enhanced geothermal system, known as Earth Source Heating, designed to meet campus heating needs.\n",
      "In 2023, Cornell was the first university in the nation to commit to Kyoto Protocol emission reductions. The same year, a concert held at Barton Hall by Dead & Company raised $3.1 million for MusiCares and the Cornell 2030 Project, two organizations which have contributed to the establishment of the Climate Solutions Fund and aims to catalyze large-scale, impactful climate research across the university, which will be administered by the Atkinson Center.\n"
     ]
    }
   ],
   "source": [
    "print(documents[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Split the document</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 100\n",
    "chunk_overlap = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = chunk_size,\n",
    "    chunk_overlap = chunk_overlap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cornell University\n",
      "20th Century History\n",
      "-----\n",
      "In 1967, Cornell experienced a fire in the Residential Club dormitory that killed eight students\n",
      "-----\n",
      "and one professor. In the late 1960s, Cornell was among the Ivy League universities that\n",
      "-----\n",
      "experienced heightened student activism related to cultural issues, civil rights, and opposition to\n",
      "-----\n",
      "U.S. involvement in the Vietnam War. In 1969, armed anti-Vietnam War protesters occupied Willard\n",
      "-----\n",
      "Straight Hall, an incident that led to a restructuring of the university's governance and forced\n",
      "-----\n",
      "the resignation of then Cornell president James Alfred Perkins.\n",
      "-----\n",
      "Since the 20th century, rankings of universities and colleges, Cornell University and its academic\n",
      "-----\n",
      "programs have routinely ranked among the best in the world. In 1995, the National Research Council\n",
      "-----\n",
      "ranked Cornell's Ph.D. programs as sixth-best in the nation. It also ranked the academic quality of\n",
      "-----\n",
      "18 individual Cornell Ph.D. programs among the top ten in the nation, which included astrophysics\n",
      "-----\n",
      "(ninth-best), chemistry (sixth-best), civil engineering (sixth-best), comparative literature\n",
      "-----\n",
      "(sixth-best), computer science (fifth-best), ecology (fourth-best), electrical engineering\n",
      "-----\n",
      "(seventh-best), English (seventh-best), French (eighth-best), geosciences (tenth-best), German\n",
      "-----\n",
      "(third-best), linguistics (ninth-best), materials science (third-best), mechanical engineering\n",
      "-----\n",
      "(seventh-best), philosophy (ninth-best), physics (sixth-best), Spanish (eighth-best), and\n",
      "-----\n",
      "statistics/biostatistics (fourth-best). The council ranked Cornell's College of Arts and Humanities\n",
      "-----\n",
      "faculty as fifth-best in the nation, its Mathematics and Physical Sciences faculty as sixth-best,\n",
      "-----\n",
      "and its College of Engineering as fifth-best.\n",
      "-----\n",
      "Natural Surroundings\n",
      "-----\n",
      "Cornell University's main campus in Ithaca is located in the Finger Lakes region in upstate New\n",
      "-----\n",
      "York and features views of the city, Cayuga Lake, and surrounding valleys. The campus is bordered\n",
      "-----\n",
      "by two gorges, Fall Creek Gorge and Cascadilla Gorge. The gorges are popular swimming spots during\n",
      "-----\n",
      "warmer months, but their use is discouraged by the university and city code due to potential safety\n",
      "-----\n",
      "hazards. Adjacent to the main campus, Cornell owns the 2,800-acre (1,100 ha) Cornell Botanic\n",
      "-----\n",
      "Gardens, which features various plants, trees, and ponds.\n",
      "-----\n",
      "Sustainability\n",
      "-----\n",
      "Cornell University has implemented several green initiatives, designed to promote sustainability\n",
      "-----\n",
      "and reduce environmental impact, including a gas-fired combined heat and power facility,an\n",
      "-----\n",
      "on-campus hydroelectric plant, and a lake source cooling system. In 2007, Cornell established a\n",
      "-----\n",
      "Center for a Sustainable Future. The same year, following a multiyear, cross-campus discussion\n",
      "-----\n",
      "about energy and sustainability, Cornell's Atkinson Center for Sustainability was established,\n",
      "-----\n",
      "funded by an $80 million gift from alumnus David R. Atkinson ('60) and his wife Patricia, the\n",
      "-----\n",
      "largest gift ever received by Cornell from an individual at the time. A subsequent $30 million\n",
      "-----\n",
      "commitment in 2021 will name a new multidisciplinary building on campus.\n",
      "-----\n",
      "As of 2020, the university, which has committed to achieving net carbon neutrality by 2035 is\n",
      "-----\n",
      "powered by six solar farms, which provide 28 megawatts of power. Cornell is developing an enhanced\n",
      "-----\n",
      "geothermal system, known as Earth Source Heating, designed to meet campus heating needs.\n",
      "-----\n",
      "In 2023, Cornell was the first university in the nation to commit to Kyoto Protocol emission\n",
      "-----\n",
      "reductions. The same year, a concert held at Barton Hall by Dead & Company raised $3.1 million for\n",
      "-----\n",
      "MusiCares and the Cornell 2030 Project, two organizations which have contributed to the\n",
      "-----\n",
      "establishment of the Climate Solutions Fund and aims to catalyze large-scale, impactful climate\n",
      "-----\n",
      "research across the university, which will be administered by the Atkinson Center.\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "for chunk in chunks:\n",
    "    print(chunk.page_content)\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index chunks into a vector db (ChromaDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': '{\"error\": \"/embeddings: Invalid model name passed in model=text-embedding-ada-002. Call `/v1/models` to view available models for your key.\"}', 'type': 'None', 'param': 'None', 'code': '400'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[100], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#vectorstore = Chroma.from_documents(documents=chunks, embedding=AzureOpenAIEmbeddings(model=\"text-embedding-3-large\"))\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m \u001b[43mChroma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAzureOpenAIEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-embedding-ada-002\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/CTI_RAG/.venv/lib/python3.11/site-packages/langchain_chroma/vectorstores.py:1128\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m   1127\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m-> 1128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/CTI_RAG/.venv/lib/python3.11/site-packages/langchain_chroma/vectorstores.py:1089\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m   1083\u001b[0m         chroma_collection\u001b[38;5;241m.\u001b[39madd_texts(\n\u001b[1;32m   1084\u001b[0m             texts\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m batch[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m [],\n\u001b[1;32m   1085\u001b[0m             metadatas\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m batch[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1086\u001b[0m             ids\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   1087\u001b[0m         )\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1089\u001b[0m     \u001b[43mchroma_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chroma_collection\n",
      "File \u001b[0;32m~/CTI_RAG/.venv/lib/python3.11/site-packages/langchain_chroma/vectorstores.py:508\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[0;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 508\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[1;32m    512\u001b[0m     length_diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(texts) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "File \u001b[0;32m~/CTI_RAG/.venv/lib/python3.11/site-packages/langchain_openai/embeddings/base.py:588\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[0;34m(self, texts, chunk_size)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[1;32m    587\u001b[0m engine \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeployment)\n\u001b[0;32m--> 588\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/CTI_RAG/.venv/lib/python3.11/site-packages/langchain_openai/embeddings/base.py:483\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[0;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[1;32m    481\u001b[0m batched_embeddings: List[List[\u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[0;32m--> 483\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_chunk_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invocation_params\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    487\u001b[0m         response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mmodel_dump()\n",
      "File \u001b[0;32m~/CTI_RAG/.venv/lib/python3.11/site-packages/openai/resources/embeddings.py:124\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[0;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    118\u001b[0m         embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[1;32m    119\u001b[0m             base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m         )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/CTI_RAG/.venv/lib/python3.11/site-packages/openai/_base_client.py:1278\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1266\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1273\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1274\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1275\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1276\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1277\u001b[0m     )\n\u001b[0;32m-> 1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/CTI_RAG/.venv/lib/python3.11/site-packages/openai/_base_client.py:955\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    953\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 955\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/CTI_RAG/.venv/lib/python3.11/site-packages/openai/_base_client.py:1059\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1056\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1058\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1059\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1062\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1063\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1067\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1068\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': '{\"error\": \"/embeddings: Invalid model name passed in model=text-embedding-ada-002. Call `/v1/models` to view available models for your key.\"}', 'type': 'None', 'param': 'None', 'code': '400'}}"
     ]
    }
   ],
   "source": [
    "#vectorstore = Chroma.from_documents(documents=chunks, embedding=AzureOpenAIEmbeddings(model=\"text-embedding-3-large\"))\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=AzureOpenAIEmbeddings(model=\"text-embedding-ada-002\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorstore' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvectorstore\u001b[49m\u001b[38;5;241m.\u001b[39msimilarity_search(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAmanita phalloides\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vectorstore' is not defined"
     ]
    }
   ],
   "source": [
    "vectorstore.similarity_search(\"Amanita phalloides\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that providers implement different scores; Chroma here\n",
    "# returns a distance metric that should vary inversely with similarity.\n",
    "vectorstore.similarity_search_with_score(\"Amanita phalloides\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare prompt (Augmentation Step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "    You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. \n",
    "    If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "    \n",
    "    Question: {question} \n",
    "    \n",
    "    Context: {context} \n",
    "    \n",
    "    Answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "retrieved_docs = retriever.invoke(\"Amanita phalloides\")\n",
    "\n",
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_docs(retriever.invoke(\"Amanita phalloides\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke(\"tell me about Amanita phalloides\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Aware Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_similarity(text1, text2):\n",
    "    template = \"\"\"\n",
    "    Analyze the contextual relationship between the following two texts:\n",
    "    \n",
    "    Text 1: {text1}\n",
    "    Text 2: {text2}\n",
    "    \n",
    "    Evaluate whether Text 2 completes or extends the context of Text 1, or if they are separate and unrelated. Assign a float score from 0 to 1, where:\n",
    "    \n",
    "    0 = The texts are entirely unrelated and should be split\n",
    "    1 = The texts are strongly connected and belong to the same context\n",
    "    \n",
    "    Consider factors such as:\n",
    "    \n",
    "    Thematic continuity\n",
    "    Logical flow\n",
    "    Shared subject matter\n",
    "    Narrative or argumentative progression\n",
    "    Linguistic cohesion\n",
    "    Provide only a single float value between 0 and 1 as your response, with up to two decimal places. For example: 0.75\n",
    "    \n",
    "    Ensure your answer contains nothing but the float value. Double-check your response before submitting\"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate.from_template(template)\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    similarity = chain.invoke({\"text1\": text1, \"text2\": text2})\n",
    "    return float(similarity.replace('.\\n\\n', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_similarity(\"\"\"The Amanita phalloides has a large and imposing epigeous (above ground) fruiting body (basidiocrap).\"\"\",\n",
    "               \"\"\"A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all white.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_metadata(metadatas):\n",
    "    merged_metadata = {}\n",
    "    for metadata in metadatas:\n",
    "        for key, value in metadata.items():\n",
    "            if key in merged_metadata:\n",
    "                merged_metadata[key] += \" \" + value  \n",
    "            else:\n",
    "                merged_metadata[key] = value\n",
    "    return merged_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "def context_text_splitter_with_llm(documents, step_size, chunk_size, max_chunk_size):\n",
    "    # Split the text\n",
    "    # Ensure you have RecursiveCharacterTextSplitter defined and available\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=step_size, chunk_overlap=0)\n",
    "\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "\n",
    "    step_chunks = [doc.page_content for doc in docs]\n",
    "    step_metadata = [doc.metadata for doc in docs]\n",
    "\n",
    "    merged_chunks = []\n",
    "    merged_metadata_chunks = []\n",
    "\n",
    "    while len(step_chunks) > 0:\n",
    "        if len(''.join(step_chunks)) < chunk_size:\n",
    "            break\n",
    "        \n",
    "        current_chunk = ''.join(step_chunks[:chunk_size//step_size])\n",
    "        current_metadata = merge_metadata(step_metadata[:chunk_size//step_size])\n",
    "        \n",
    "        step_chunks = step_chunks[chunk_size//step_size:]\n",
    "        step_metadata = step_metadata[chunk_size//step_size:]\n",
    "\n",
    "        chunk_appended = False\n",
    "\n",
    "        while len(step_chunks) > 0:\n",
    "            next_step_chunk = step_chunks.pop(0)\n",
    "            next_step_chunk_metadata = step_metadata.pop(0)\n",
    "\n",
    "            similarity_score = llm_similarity(current_chunk, next_step_chunk)\n",
    "\n",
    "            if similarity_score > 0.49 and len(current_chunk) + len(next_step_chunk) <= max_chunk_size:\n",
    "                current_chunk += \" \" + next_step_chunk\n",
    "                current_metadata = merge_metadata([current_metadata, next_step_chunk_metadata])\n",
    "            else:\n",
    "                merged_chunks.append(\" \" + current_chunk)\n",
    "                merged_metadata_chunks.append(current_metadata)\n",
    "                \n",
    "                chunk_appended = True\n",
    "                \n",
    "                step_chunks.insert(0, next_step_chunk)\n",
    "                step_metadata.insert(0, next_step_chunk_metadata)\n",
    "                \n",
    "                break\n",
    "\n",
    "        if not chunk_appended:\n",
    "            merged_chunks.append(\" \" + current_chunk)\n",
    "            merged_metadata_chunks.append(current_metadata)\n",
    "\n",
    "    if len(step_chunks) > 0:\n",
    "        merged_chunks.append(' '.join(step_chunks))\n",
    "        merged_metadata_chunks.append(merge_metadata(step_metadata))\n",
    "\n",
    "    merged_docs = []\n",
    "    for chunk, metadata in zip(merged_chunks, merged_metadata_chunks):\n",
    "        merged_docs.append(Document(page_content=chunk, metadata=metadata))\n",
    "\n",
    "    return merged_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = context_text_splitter_with_llm(documents, 100, 200, 1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chunks:\n",
    "    print(chunk.page_content)\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index into Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "client = chromadb.Client()\n",
    "\n",
    "# Access a specific collection\n",
    "collection_name = \"langchain\"\n",
    "client.delete_collection(collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=AzureOpenAIEmbeddings(model=\"text-embedding-3-large\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke(\"tell me about Amanita phalloides\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Vector Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = AzureOpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = embeddings.embed_documents(texts=[\"Apple\"])[0]\n",
    "v2 = embeddings.embed_documents(texts=[\"Orange\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Compute the cosine similarity between two vectors using SciPy.\"\"\"\n",
    "    return 1 - cosine(vec1, vec2)  # cosine function from SciPy computes the distance, not similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "def context_text_splitter(documents, step_size, chunk_size, max_chunk_size):\n",
    "    # Split the text\n",
    "    # Ensure you have RecursiveCharacterTextSplitter defined and available\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=step_size, chunk_overlap=0)\n",
    "\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "\n",
    "    step_chunks = [doc.page_content for doc in docs]\n",
    "    step_metadata = [doc.metadata for doc in docs]\n",
    "\n",
    "    merged_chunks = []\n",
    "    merged_metadata_chunks = []\n",
    "\n",
    "    while len(step_chunks) > 0:\n",
    "        if len(''.join(step_chunks)) < chunk_size:\n",
    "            break\n",
    "        \n",
    "        current_chunk = ''.join(step_chunks[:chunk_size//step_size])\n",
    "        current_metadata = merge_metadata(step_metadata[:chunk_size//step_size])\n",
    "        \n",
    "        step_chunks = step_chunks[chunk_size//step_size:]\n",
    "        step_metadata = step_metadata[chunk_size//step_size:]\n",
    "\n",
    "        chunk_appended = False\n",
    "\n",
    "        while len(step_chunks) > 0:\n",
    "            next_step_chunk = step_chunks.pop(0)\n",
    "            next_step_chunk_metadata = step_metadata.pop(0)\n",
    "\n",
    "            similarity_score = cosine_similarity(embeddings.embed_query(current_chunk), embeddings.embed_query(next_step_chunk))\n",
    "\n",
    "            if similarity_score > 0.79 and len(current_chunk) + len(next_step_chunk) <= max_chunk_size:\n",
    "                current_chunk += \" \" + next_step_chunk\n",
    "                current_metadata = merge_metadata([current_metadata, next_step_chunk_metadata])\n",
    "            else:\n",
    "                merged_chunks.append(\" \" + current_chunk)\n",
    "                merged_metadata_chunks.append(current_metadata)\n",
    "                \n",
    "                chunk_appended = True\n",
    "                \n",
    "                step_chunks.insert(0, next_step_chunk)\n",
    "                step_metadata.insert(0, next_step_chunk_metadata)\n",
    "                \n",
    "                break\n",
    "\n",
    "        if not chunk_appended:\n",
    "            merged_chunks.append(\" \" + current_chunk)\n",
    "            merged_metadata_chunks.append(current_metadata)\n",
    "\n",
    "    if len(step_chunks) > 0:\n",
    "        merged_chunks.append(' '.join(step_chunks))\n",
    "        merged_metadata_chunks.append(merge_metadata(step_metadata))\n",
    "\n",
    "    merged_docs = []\n",
    "    for chunk, metadata in zip(merged_chunks, merged_metadata_chunks):\n",
    "        merged_docs.append(Document(page_content=chunk, metadata=metadata))\n",
    "\n",
    "    return merged_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = context_text_splitter(documents, 100, 200, 1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chunks:\n",
    "    print(chunk.page_content)\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
